{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danieloliveiradebrito/Projetos/fakenews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieloliveiradebrito/Projetos/fakenews/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fakenews.read_data import read_fake_recogna\n",
    "\n",
    "df = read_fake_recogna().to_pandas()\n",
    "\n",
    "train_full, test = train_test_split(df,\n",
    "                              test_size=0.2,\n",
    "                              random_state=42,\n",
    "                              shuffle=True,\n",
    "                              stratify=df[\"label\"]\n",
    "                            )\n",
    "\n",
    "train, val = train_test_split(train_full,\n",
    "                              test_size=0.2,\n",
    "                              random_state=42,\n",
    "                              shuffle=True,\n",
    "                              stratify=train_full[\"label\"]\n",
    "                            )\n",
    "\n",
    "ds = datasets.DatasetDict()\n",
    "ds[\"train\"] = datasets.Dataset.from_pandas(train.reset_index(drop=True))\n",
    "ds[\"test\"] = datasets.Dataset.from_pandas(test.reset_index(drop=True))\n",
    "ds[\"validation\"] = datasets.Dataset.from_pandas(val.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at adalbertojunior/distilbert-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"adalbertojunior/distilbert-portuguese-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"adalbertojunior/distilbert-portuguese-cased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at adalbertojunior/distilbert-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 7616/7616 [00:03<00:00, 2349.36 examples/s]\n",
      "Map: 100%|██████████| 2381/2381 [00:01<00:00, 2373.91 examples/s]\n",
      "Map: 100%|██████████| 1905/1905 [00:00<00:00, 2416.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from fakenews.train import FakeNewsTrainer\n",
    "\n",
    "fake_recogna_trainer = FakeNewsTrainer(ds)\n",
    "tokenized_ds = fake_recogna_trainer.tokenize_ds()\n",
    "trainer = fake_recogna_trainer.get_trainer(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 500/2856 [04:36<21:16,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3086, 'grad_norm': 0.08902580291032791, 'learning_rate': 1.649859943977591e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 952/2856 [09:22<17:15,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19316671788692474, 'eval_accuracy': 0.946981627296588, 'eval_runtime': 39.4993, 'eval_samples_per_second': 48.229, 'eval_steps_per_second': 6.051, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1000/2856 [09:52<17:01,  1.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2132, 'grad_norm': 0.10201774537563324, 'learning_rate': 1.2997198879551822e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1500/2856 [14:33<12:54,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1253, 'grad_norm': 16.925861358642578, 'learning_rate': 9.49579831932773e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 1904/2856 [19:01<09:32,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1953076273202896, 'eval_accuracy': 0.9559055118110236, 'eval_runtime': 36.057, 'eval_samples_per_second': 52.833, 'eval_steps_per_second': 6.628, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2000/2856 [19:57<07:49,  1.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1034, 'grad_norm': 0.31797081232070923, 'learning_rate': 5.994397759103642e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2500/2856 [24:45<03:24,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0591, 'grad_norm': 0.01950586959719658, 'learning_rate': 2.492997198879552e-06, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2856/2856 [28:43<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2224578559398651, 'eval_accuracy': 0.958005249343832, 'eval_runtime': 35.375, 'eval_samples_per_second': 53.852, 'eval_steps_per_second': 6.756, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2856/2856 [28:45<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1725.6626, 'train_samples_per_second': 13.24, 'train_steps_per_second': 1.655, 'train_loss': 0.14849792608693868, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2856, training_loss=0.14849792608693868, metrics={'train_runtime': 1725.6626, 'train_samples_per_second': 13.24, 'train_steps_per_second': 1.655, 'total_flos': 3026615124492288.0, 'train_loss': 0.14849792608693868, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:46<00:00,  6.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16764046251773834,\n",
       " 'eval_accuracy': 0.9546409071818563,\n",
       " 'eval_runtime': 47.6348,\n",
       " 'eval_samples_per_second': 49.984,\n",
       " 'eval_steps_per_second': 6.256,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4998)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.mean(tokenized_ds[\"test\"][\"labels\"].float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
