{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danieloliveiradebrito/Projetos/fakenews\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apagão vaticano papar presar acusação tráfico criança e fraude .. o papar francisco tuitou manhã o curador beaver confirmar o papar francisco equipar rede social agendar publicação e planejada antecedência clicar tweet o tweetdeck agendar o publicação.o papar francisco custódio federal itália vestir vídeo o fbi o interrogatório fontes.oficiais militar policiar italiano e unidade crime sexual o casar papar vaticano prender e policiar alto e colocar prisão incidente pessoa relatar ouvir tiro o polícia confirmar atirar armas.o papar francisco encontra-se atualmente prisão desconhecido interrogar agente federar trabalhar o itália e interpol acordar relatório o fbi arranjar voar e interrogá-lo o interpol terminar ele.giuseppe governale o procurador-chefe antimafia itália chamar comedir pesquisar o papar francisco e vaticano o ′′ subestimar ′′ e particularmente perigoso dever capacidade proliferar nação e infiltrar n realizar o prisão policiar e militar cortar o energia vaticano atenuar o câmera o o viver notar web-sleuths rapidamente começar o suspeitar acontecer vaticano'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.read_excel(\"data/raw/FakeRecogna.xlsx\").select([\"Noticia\"]).head(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bomba lira fraudar o votação brasília explodir votar impresso auditável perder roubar umar haver fraudar o votação o câmara deputar imaginar o eleição presidencial bomba bomba bomba bomba lira fraudar o votação brasília explodir'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_excel(\"data/raw/FakeRecogna_no_removal_words.xlsx\").select([\"Noticia\"]).head(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 1 - DistilBert BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at adalbertojunior/distilbert-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"adalbertojunior/distilbert-portuguese-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"adalbertojunior/distilbert-portuguese-cased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9521\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2381\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = (pl.read_excel(\"data/raw/FakeRecogna.xlsx\")\n",
    " .select([\"Noticia\", \"Classe\"])\n",
    " .rename({\"Noticia\": \"text\", \"Classe\": \"label\"})\n",
    " .to_pandas()\n",
    ")\n",
    "\n",
    "train, test = train_test_split(df,\n",
    "                              test_size=0.2,\n",
    "                              random_state=42,\n",
    "                              shuffle=True,\n",
    "                              stratify=df[\"label\"]\n",
    "                            )\n",
    "\n",
    "ds = datasets.DatasetDict()\n",
    "ds[\"train\"] = datasets.Dataset.from_pandas(train.reset_index(drop=True))\n",
    "ds[\"test\"] = datasets.Dataset.from_pandas(test.reset_index(drop=True))\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Via HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9521/9521 [00:03<00:00, 2500.87 examples/s]\n",
      "Map: 100%|██████████| 2381/2381 [00:00<00:00, 2539.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    acc = accuracy_score(pred.label_ids, pred.predictions.argmax(1))\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_function)\n",
    "tokenized_ds = tokenized_ds.remove_columns([\"text\"])\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "tokenized_ds = tokenized_ds.with_format(\"torch\")\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    push_to_hub=False,\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 500/3573 [04:39<27:59,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3458, 'grad_norm': 29.037113189697266, 'learning_rate': 1.7201231458158412e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1000/3573 [09:19<23:39,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1897, 'grad_norm': 2.56915545463562, 'learning_rate': 1.4402462916316821e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 1500/3573 [14:06<19:31,  1.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1293, 'grad_norm': 0.031020155176520348, 'learning_rate': 1.1603694374475232e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 2000/3573 [18:49<15:20,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0963, 'grad_norm': 0.013136681169271469, 'learning_rate': 8.804925832633642e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 2500/3573 [23:33<10:03,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1087, 'grad_norm': 0.013243364170193672, 'learning_rate': 6.006157290792052e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 3000/3573 [28:19<05:19,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0497, 'grad_norm': 1.1427842378616333, 'learning_rate': 3.207388748950462e-06, 'epoch': 2.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3500/3573 [33:02<00:40,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0452, 'grad_norm': 59.73691940307617, 'learning_rate': 4.0862020710887213e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3573/3573 [33:44<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2024.0105, 'train_samples_per_second': 14.112, 'train_steps_per_second': 1.765, 'train_loss': 0.1358955533705337, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3573, training_loss=0.1358955533705337, metrics={'train_runtime': 2024.0105, 'train_samples_per_second': 14.112, 'train_steps_per_second': 1.765, 'total_flos': 3783666307811328.0, 'train_loss': 0.1358955533705337, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:50<00:00,  5.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14498692750930786,\n",
       " 'eval_accuracy': 0.9701805963880722,\n",
       " 'eval_runtime': 51.7588,\n",
       " 'eval_samples_per_second': 46.002,\n",
       " 'eval_steps_per_second': 5.757,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/distilbert-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o imagem virar e mexer voltar o aparecer o redar social n gatar branco mostrar espertar o cão o suar procurar esconder murar e espremer portão\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_teste = test.query(\"label==0\")[\"text\"].sample().values[0]\n",
    "print(sample_teste)\n",
    "encoding = tokenizer(sample_teste, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k, v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)\n",
    "logits = outputs.logits\n",
    "probs = torch.nn.Sigmoid()(logits.squeeze().cpu())\n",
    "probs.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Via TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(train[\"text\"])\n",
    "X_test = tfidf.transform(test[\"text\"])\n",
    "\n",
    "y_train, y_test = train[\"label\"], test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340613187736245"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custo processamento todo o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.014337"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_por_palavra = 2\n",
    "custo_usd_por_mm_token = 0.5\n",
    "total_palavras = df[\"text\"].apply(lambda x: len(x.split())).sum()\n",
    "total_tokens = total_palavras * token_por_palavra\n",
    "total_tokens * custo_usd_por_mm_token / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "_ = load_dotenv()\n",
    "openai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Brazil is Brasília.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Brazil?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "response.model_dump()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "amostra_teste = test.sample(n_samples)\n",
    "textos = amostra_teste[\"text\"].values\n",
    "labels = amostra_teste[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Você é um jornalista brasileiro com amplo conhecimento em notícias falsas.\n",
    "Você foi desenhado para gerar outputs inteiros (0, 1).\n",
    "Por favor, classifique a notícia abaixo como verdadeira ou falsa.\n",
    "Tente avaliar a notícia da melhor maneira possível, não solicitando por novas informações.\n",
    "Retorne '1' se considerar a notícia falsa, '0' se considerar verdadeira. Por exemplo:\n",
    "\n",
    "Notícia: _____\n",
    "Assistente: 1\n",
    "\"\"\"\n",
    "temperatura = 0\n",
    "\n",
    "respostas = []\n",
    "for texto in textos:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    # response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": texto,}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    pred = response.model_dump()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    if int(pred) not in [0, 1]:\n",
    "        pred = np.nan\n",
    "    \n",
    "    respostas.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(i) for i in respostas]) #maioria respostas consideradas como fraude (texto zoado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot\n",
    "k = 20 (10 de cada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/kl0p__nj01dd11mtnll0179r0000gn/T/ipykernel_42713/2210678676.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample_train = train.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(10)).sample(frac=1).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "sample_train = train.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(10)).sample(frac=1).reset_index(drop=True)\n",
    "train_text = sample_train[\"text\"]\n",
    "train_label = sample_train[\"label\"]\n",
    "\n",
    "inicio_prompt = \"\"\"\n",
    "Você é um jornalista brasileiro com amplo conhecimento em notícias falsas.\n",
    "Você foi desenhado para gerar outputs inteiros (0, 1).\n",
    "Por favor, classifique a notícia abaixo como verdadeira ou falsa.\n",
    "Tente avaliar a notícia da melhor maneira possível, não solicitando por novas informações.\n",
    "Retorne '1' se considerar a notícia falsa, '0' se considerar verdadeira. Por exemplo:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "exemplos = \"\"\n",
    "\n",
    "for texto, label in zip(train_text, train_label):\n",
    "    exemplos += f\"\"\"\n",
    "###\n",
    "Notícia: {texto}\n",
    "Assistente: {label}\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "fim_prompt = \"\"\"\n",
    "####\n",
    "Notícia: \n",
    "\"\"\"\n",
    "\n",
    "prompt_few_shot = (inicio_prompt + exemplos + fim_prompt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatura = 0\n",
    "\n",
    "respostas_few_shot = []\n",
    "for texto in textos:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    # response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_few_shot},\n",
    "            {\"role\": \"user\", \"content\": texto,}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    pred = response.model_dump()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    if int(pred) not in [0, 1]:\n",
    "        pred = np.nan\n",
    "    \n",
    "    respostas_few_shot.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, [int(i) for i in respostas_few_shot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, [int(i) for i in respostas])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
